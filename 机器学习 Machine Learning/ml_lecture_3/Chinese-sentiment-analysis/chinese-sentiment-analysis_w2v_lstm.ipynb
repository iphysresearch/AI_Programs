{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用LSTM做情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "#用于词向量训练\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#容易上手的keras，为了保持和Aaron老师用的DL库一致...\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout,Activation\n",
    "from keras.models import model_from_yaml\n",
    "np.random.seed(1337)  # For Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.setrecursionlimit(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设定超参数\n",
    "vocab_dim = 100\n",
    "maxlen = 100\n",
    "n_iterations = 1  # 可以设定高一点...\n",
    "n_exposures = 10\n",
    "window_size = 7\n",
    "batch_size = 32\n",
    "n_epoch = 4\n",
    "input_length = 100\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载训练文件\n",
    "def loadfile():\n",
    "    neg=pd.read_excel('data/neg.xls',header=None,index=None)\n",
    "    pos=pd.read_excel('data/pos.xls',header=None,index=None)\n",
    "\n",
    "    combined=np.concatenate((pos[0], neg[0]))\n",
    "    y = np.concatenate((np.ones(len(pos),dtype=int), np.zeros(len(neg),dtype=int)))\n",
    "\n",
    "    return combined,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对句子经行分词，并去掉换行符\n",
    "def tokenizer(text):\n",
    "    ''' Simple Parser converting each document to lower-case, then\n",
    "        removing the breaks for new lines and finally splitting on the\n",
    "        whitespace\n",
    "    '''\n",
    "    text = [jieba.lcut(document.replace('\\n', '')) for document in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.vocab.keys(),\n",
    "                            allow_update=True)\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引\n",
    "        w2vec = {word: model[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量\n",
    "\n",
    "        def parse_dataset(combined):\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data\n",
    "        combined=parse_dataset(combined)\n",
    "        combined= sequence.pad_sequences(combined, maxlen=maxlen)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "        return w2indx, w2vec,combined\n",
    "    else:\n",
    "        print 'No data provided...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def word2vec_train(combined):\n",
    "\n",
    "    model = Word2Vec(size=vocab_dim,\n",
    "                     min_count=n_exposures,\n",
    "                     window=window_size,\n",
    "                     workers=cpu_count,\n",
    "                     iter=n_iterations)\n",
    "    model.build_vocab(combined)\n",
    "    model.train(combined)\n",
    "    model.save('lstm_model_data/model/Word2vec_model.pkl')\n",
    "    index_dict, word_vectors,combined = create_dictionaries(model=model,combined=combined)\n",
    "    return   index_dict, word_vectors,combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(index_dict,word_vectors,combined,y):\n",
    "\n",
    "    n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))#索引为0的词语，词向量全为0\n",
    "    for word, index in index_dict.items():#从索引为1的词语开始，对每个词语对应其词向量\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n",
    "    print x_train.shape,y_train.shape\n",
    "    return n_symbols,embedding_weights,x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##定义网络结构\n",
    "def train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test):\n",
    "    print 'Defining a Simple Keras Model...'\n",
    "    model = Sequential()  # or Graph or whatever\n",
    "    model.add(Embedding(output_dim=vocab_dim,\n",
    "                        input_dim=n_symbols,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=input_length))  # Adding Input Length\n",
    "    model.add(LSTM(output_dim=50, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print 'Compiling the Model...'\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    print \"Train...\"\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=n_epoch,verbose=1, validation_data=(x_test, y_test),show_accuracy=True)\n",
    "\n",
    "    print \"Evaluate...\"\n",
    "    score = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    yaml_string = model.to_yaml()\n",
    "    with open('lstm_model_data/model/lstm.yml', 'w') as outfile:\n",
    "        outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n",
    "    model.save_weights('lstm_model_data/model/lstm.h5')\n",
    "    print 'Test score:', score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练模型，并保存\n",
    "def train():\n",
    "    print 'Loading Data...'\n",
    "    combined,y=loadfile()\n",
    "    print len(combined),len(y)\n",
    "    print 'Tokenising...'\n",
    "    combined = tokenizer(combined)\n",
    "    print 'Training a Word2vec model...'\n",
    "    index_dict, word_vectors,combined=word2vec_train(combined)\n",
    "    print 'Setting up Arrays for Keras Embedding Layer...'\n",
    "    n_symbols,embedding_weights,x_train,y_train,x_test,y_test=get_data(index_dict, word_vectors,combined,y)\n",
    "    print x_train.shape,y_train.shape\n",
    "    train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_transform(string):\n",
    "    words=jieba.lcut(string)\n",
    "    words=np.array(words).reshape(1,-1)\n",
    "    model=Word2Vec.load('lstm_model_data/model/Word2vec_model.pkl')\n",
    "    _,_,combined=create_dictionaries(model,words)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_predict(string):\n",
    "    print 'loading model......'\n",
    "    with open('lstm_model_data/model/lstm.yml', 'r') as f:\n",
    "        yaml_string = yaml.load(f)\n",
    "    model = model_from_yaml(yaml_string)\n",
    "\n",
    "    print 'loading weights......'\n",
    "    model.load_weights('lstm_model_data/model/lstm.h5')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "    data=input_transform(string)\n",
    "    data.reshape(1,-1)\n",
    "    #print data\n",
    "    result=model.predict_classes(data)\n",
    "    if result[0][0]==1:\n",
    "        print string,' positive'\n",
    "    else:\n",
    "        print string,' negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train()\n",
    "string='电池充完了电连手机都打不开.简直烂的要命.真是金玉其外,败絮其中!连5号电池都不如'\n",
    "string='牛逼的手机，从3米高的地方摔下去都没坏，质量非常好'\n",
    "string='酒店的环境非常好，价格也便宜，值得推荐'\n",
    "string='手机质量太差了，傻逼店家，赚黑心钱，以后再也不会买了'\n",
    "string='我是傻逼'\n",
    "string='你是傻逼'\n",
    "string='屏幕较差，拍照也很粗糙。'\n",
    "string='质量不错，是正品 ，安装师傅也很好，才要了83元材料费'\n",
    "string='东西非常不错，安装师傅很负责人，装的也很漂亮，精致，谢谢安装师傅！'\n",
    "\n",
    "lstm_predict(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
