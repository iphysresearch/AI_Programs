{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL是架在Spark core之上允许我们用SQL去操作大型数据库的高级层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先你需要了解的是，**我们需要在之前的`SparkContext`基础上架上一层`SQLContext`，然后才能完成后续的SQL。**<br>\n",
    "如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SQLContext, Row\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL得到的RDD，可以转成类似pandas中大家熟悉的dataframe格式。<br>\n",
    "对的，大家依旧可以把它想象成Excel中的图表，每个item都是一个`Row`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的初始化`Dataframe`方法是先用`sc.parallelize`产出一个RDD，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\n",
    "        Row(name='John', home='Brussels', age=35),\n",
    "        Row(name='Jack', home='Brussels', age=32),\n",
    "        Row(name='Jane', home='Leuven', age=42),\n",
    "        Row(name='Jill', home='Mechelen', age=53),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后生成一个dataframe："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlCtx.createDataFrame(rdd)\n",
    "df.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，你也可以在Hive表里使用Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "刚才的table建好后，你就可以在上面跑各种SQL了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[home: string, mean: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgAge = sqlCtx.sql(\n",
    "    \"\"\"SELECT\n",
    "           home,\n",
    "           AVG(age) AS mean\n",
    "       FROM people\n",
    "       GROUP BY home\"\"\"\n",
    ")\n",
    "avgAge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家都知道在一般的数据表里执行SQL之后得到的还是一份表格式的数据，在Spark SQL执行完之后，你得到的依旧是一个SchemaRDD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(home=u'Mechelen', mean=53.0),\n",
       " Row(home=u'Leuven', mean=42.0),\n",
       " Row(home=u'Brussels', mean=33.5)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgAge.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "别忘了学习到的RDD上的transform或者action等operation，**同时你可以用`row.fieldname`取出来某个field**，如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average age in Mechelen is 53.0 years\n",
      "Average age in Leuven is 42.0 years\n",
      "Average age in Brussels is 33.5 years\n"
     ]
    }
   ],
   "source": [
    "print(avgAge.rdd\n",
    "      .map(lambda row: \"Average age in {0} is {1} years\"\n",
    "                        .format(row.home, row.mean))\n",
    "      .reduce(lambda x, y: x + \"\\n\" + y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>练习作业</font><br>\n",
    "**回顾一下，我们可以在pandas里定义这样一个dataframe**\n",
    "```\n",
    "data = {\n",
    "    'country': ['BE', 'BE', 'BE', 'NL', 'NL', 'NL'],\n",
    "    'year': [1913, 1950, 2003, 1913, 1950, 2003],\n",
    "    'gdp_per_capita': [4220, 5462, 21205, 4049, 5996, 21480]\n",
    "}\n",
    "frame = DataFrame(data)\n",
    "```\n",
    "**请大家在Spark中生成一个Dataframe，再做一个简单的SQL分析：求这3年各首都的平均GDP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>练习作业</font><br>\n",
    "**`sc.textFile`, `union` 和 `map`练习。 把提供的`names`数据集构建成一个Dataframe，包含`year`, `name`, `sex`和`births`，把它注册成一个表`\"names\"`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 求从1939到1945年美国总出生的人口**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 统计从1880到2014每一年叫`\"Mary\"`的宝宝出生数目，并用matplotlib绘制成图像**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 统计从1880到2014每一年男孩和女孩的出生数，并绘制在一张图中。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 统计出来每一年出生的宝宝频次最高的前1000个名字，以及它们的占比，并用绘图的方式去展示出来**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更多知识\n",
    "到此为止，应该Spark核心知识与Spark SQL知识你都有一些了解了，其实有更多的Spark SQL知识，比如:\n",
    "* Complex Joins\n",
    "* Subqueries\n",
    "* Basic math functions\n",
    "* User-defined functions (UDFs)\n",
    "* ...\n",
    "\n",
    "完整的文档可以看[这里](https://spark.apache.org/docs/2.1.0/sql-programming-guide.html#compatibility-with-apache-hive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
